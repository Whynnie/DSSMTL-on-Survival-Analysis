{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03867538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import ndimage\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper \n",
    "from lifelines.utils import concordance_index\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras import backend as k\n",
    "from sklearn.preprocessing import normalize\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import *\n",
    "from pycox.preprocessing.label_transforms import LabTransDiscreteTime\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(2021)\n",
    "\n",
    "\n",
    "df_train = pd.read_excel(\"../Melanoma_train_data_py.xls\", index_col= 0)\n",
    "df_test = pd.read_excel(\"../Melanoma_test_data_py.xls\", index_col= 0)\n",
    "\n",
    "## Dealing with Missing values\n",
    "\n",
    "#df_train['meal.cal'].fillna(df_train['meal.cal'].mean(), inplace=True)\n",
    "#df_train['wt.loss'].fillna(df_train['wt.loss'].mean(), inplace=True)\n",
    "#df_test['meal.cal'].fillna(df_test['meal.cal'].mean(), inplace=True)\n",
    "#df_test['wt.loss'].fillna(df_test['wt.loss'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "\n",
    "\n",
    "cols_standardize = ['age', \"thick\"]\n",
    "cols_leave = [x for x in df_train.columns.to_list() if x not in [\"time\", \"sex\", \"status\", \"age\", \"invasion\",\n",
    "                                                                 \"ici\", \"epicel\", \"ulcer\", \"thick\"] ]\n",
    "\n",
    "standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "x_mapper = DataFrameMapper(standardize + leave)\n",
    "\n",
    "x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "x_test = x_mapper.transform(df_test).astype('float32')\n",
    "\n",
    "\n",
    "def get_y_labels(status, time):\n",
    "    ret = np.zeros((status.shape[0], np.max(time) + 1))\n",
    "    for i in range(status.shape[0]):\n",
    "        if status[i] == 1: # csd\n",
    "            ret[i, 0:time[i] - 1 + 1] = 0\n",
    "            ret[i, time[i] - 1 + 1:] = 1\n",
    "        elif status[i] == 2: # other death\n",
    "            ret[i, 0:time[i] - 1 + 1] = 0\n",
    "            ret[i, time[i] - 1 + 1:] = 2  \n",
    "        elif status[i] == 0: # censor\n",
    "            ret[i, 0:time[i] + 1] = 0\n",
    "            ret[i, time[i] + 1:] = 3   \n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "y_train = get_y_labels(df_train['status'], df_train['time'])\n",
    "y_test = get_y_labels(df_test['status'], df_test['time'])\n",
    "\n",
    "\n",
    "time_interval = 6\n",
    "time_max = np.max(df_train['time'])\n",
    "time_length = time_max//time_interval\n",
    "\n",
    "time_max_test = np.max(df_test['time'])\n",
    "time_length_test = time_max_test//time_interval\n",
    "\n",
    "y_train = y_train[:, np.arange(time_interval, time_max, time_interval)]\n",
    "y_test = y_test[:, np.arange(time_interval, time_max_test, time_interval)]\n",
    "\n",
    "print (\"train_set_x shape: \" + str(x_train.shape))\n",
    "print (\"train_set_y shape: \" + str(y_train.shape))\n",
    "print (\"test_set_x shape: \" + str(x_test.shape))\n",
    "print (\"test_set_y shape: \" + str(y_test.shape))\n",
    "\n",
    "\n",
    "\n",
    "y_train_status = to_categorical(y_train)\n",
    "y_test_status = to_categorical(y_test)\n",
    "\n",
    "def reshape_y(y):\n",
    "    dim = y.shape[1]\n",
    "    ret = []\n",
    "    for i in range(dim):\n",
    "        ret.append(y[:, i, 0:3])\n",
    "    return ret        \n",
    "\n",
    "\n",
    "\n",
    "y_train_status = reshape_y(y_train_status)\n",
    "y_test_status = reshape_y(y_test_status)\n",
    "\n",
    "\n",
    "\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "y_train_status_f = y_train_status + [y_train] + [y_train]\n",
    "y_test_status_f = y_test_status + [y_test] + [y_test]\n",
    "\n",
    "\n",
    "\n",
    "get_target = lambda df: (df['time'].values, df['status'].values)\n",
    "durations_train, events_train = get_target(df_train)\n",
    "durations_test, events_test = get_target(df_test)\n",
    "\n",
    "\n",
    "def logloss(lambda3):\n",
    "    def loss(y_true, y_pred):\n",
    "        mask_alive = y_true[:, 0]\n",
    "        mask_dead_cause = y_true[:, 1]\n",
    "        mask_dead_other = y_true[:, 2]\n",
    "        mask_censored = 1 - (mask_alive + mask_dead_cause + mask_dead_other)\n",
    "   \n",
    "        logloss = -1 * k.mean(mask_alive * k.log(y_pred[:, 0]) + mask_dead_cause * k.log(y_pred[:, 1])\n",
    "                              + mask_dead_other * k.log(y_pred[:, 2])) - lambda3 * k.mean(y_pred[:, 1] \n",
    "                                                                                          * mask_censored \n",
    "                                                                                          * k.log(y_pred[:, 1])\n",
    "                                                                                          + y_pred[:, 0] \n",
    "                                                                                          * mask_censored \n",
    "                                                                                          * k.log(y_pred[:, 0])\n",
    "                                                                                          + y_pred[:, 2] * \n",
    "                                                                                          mask_censored * \n",
    "                                                                                          k.log(y_pred[:, 2]))\n",
    "        return logloss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def rankingloss_cause(y_true, y_pred, name = None):\n",
    "    ranking_loss = 0\n",
    "    # idx = [1,3,5,7,9,11,13,15]\n",
    "    for i in range(time_length):\n",
    "        for j in range(i + 1, time_length, 1):\n",
    "            tmp = y_pred[:, i] - y_pred[:, j]\n",
    "            tmp1 = tmp > 0\n",
    "            tmp1 = tf.cast(tmp1, tf.float32)\n",
    "            ranking_loss = ranking_loss + k.mean((tmp1 * tmp * (j - i)))\n",
    "    return ranking_loss\n",
    "\n",
    "\n",
    "def rankingloss_other(y_true, y_pred, name = None):\n",
    "    ranking_loss = 0\n",
    "    for i in range(time_length):\n",
    "        for j in range(i + 1, time_length, 1):\n",
    "            tmp = y_pred[:, i] - y_pred[:, j]\n",
    "            tmp1 = tmp > 0\n",
    "            tmp1 = tf.cast(tmp1, tf.float32)\n",
    "            ranking_loss = ranking_loss + k.mean((tmp1 * tmp * (j - i)))\n",
    "    return ranking_loss\n",
    "\n",
    "\n",
    "list_lambda3 = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "list_lambda4 = [0.01, 0.1, 1, 10, 100]\n",
    "list_lr = [0.1, 0.01, 0.001, 0.0001]\n",
    "list_batch_size = [64, 128, 256, 512, 1024]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parameters = []\n",
    "for lambda3 in list_lambda3:\n",
    "    for lambda4 in list_lambda4:\n",
    "        for lr in list_lr:\n",
    "            for batch_size in list_batch_size:\n",
    "                parameters.append([lambda3, lambda4, lr, batch_size])\n",
    "\n",
    "\n",
    "ssmtlr_cv_results = pd.DataFrame(parameters)\n",
    "ssmtlr_cv_results[\"cindex\"] = 0\n",
    "\n",
    "\n",
    "kf = KFold(n_splits = 5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for index in range(ssmtlr_cv_results.shape[0]):\n",
    "    lambda3 = ssmtlr_cv_results.iloc[index, 0]\n",
    "    lambda4 = ssmtlr_cv_results.iloc[index, 1]\n",
    "    lr = ssmtlr_cv_results.iloc[index, 2]\n",
    "    batch_size = ssmtlr_cv_results.iloc[index, 3]\n",
    "\n",
    "    cindexes = []\n",
    "    for train_index, test_index in kf.split(df_train):\n",
    "        X_tr = x_train[train_index, ]\n",
    "        X_val = x_train[test_index, ]\n",
    "\n",
    "        Y_tr_0 = y_train[train_index, ]\n",
    "        Y_val_0 = y_train[test_index, ]\n",
    "\n",
    "        Y_tr_1 = []\n",
    "        Y_val_1 = []\n",
    "        for i in range(time_length):\n",
    "            Y_tr_1.append(y_train_status[i][train_index])\n",
    "            Y_val_1.append(y_train_status[i][test_index])\n",
    "        \n",
    "        Y_tr = Y_tr_1 + [Y_tr_0]\n",
    "        Y_val = Y_val_1 + [Y_val_0]\n",
    "\n",
    "        input_tensor = Input((X_tr.shape[1],))\n",
    "        x = input_tensor\n",
    "        x = Dense(16, activation = 'sigmoid', kernel_regularizer = L1L2(l1 = 0., l2 = 0.),\n",
    "                  kernel_initializer= tf.keras.initializers.VarianceScaling())(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(8, activation = 'sigmoid', kernel_regularizer = L1L2(l1 = 0., l2 = 0.), \n",
    "                  kernel_initializer= tf.keras.initializers.VarianceScaling())(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        \n",
    "        prepare_list = {}\n",
    "        for i in range(time_length):\n",
    "             prepare_list['x' + str(i)] = Dense(3, activation = 'softmax',\n",
    "                                                kernel_regularizer = L1L2(l1 = 0., l2 = 0.), \n",
    "                                                name = 'month_' + str(i))(x)\n",
    "\n",
    "        xx1 = concatenate(list(prepare_list.values()))\n",
    "        xx2 = Lambda(lambda x: x[:, 1::3], name = 'ranking_cause')(xx1)\n",
    "        xx3 = Lambda(lambda x: x[:, 2::3], name = 'ranking_other')(xx1)\n",
    "\n",
    "\n",
    "        losses = {}\n",
    "        loss_weights = {}\n",
    "        for i in range(time_length):\n",
    "            losses['month_' + str(i)] = logloss(lambda3)\n",
    "            loss_weights['month_' + str(i)] = 1\n",
    "        losses['ranking_cause'] = rankingloss_cause\n",
    "        losses['ranking_other'] = rankingloss_other\n",
    "        loss_weights['ranking_cause'] = lambda4\n",
    "        loss_weights['ranking_other'] = lambda4\n",
    "\n",
    "        model = Model(input_tensor, list(prepare_list.values()) + [xx2] + [xx3])\n",
    "        model.compile(optimizer = Adam(lr), loss = losses, loss_weights = loss_weights)\n",
    "\n",
    "        model.fit(X_tr, Y_tr, epochs = 100, validation_data=(X_val, Y_val), \n",
    "            batch_size = batch_size, shuffle = True, verbose = 1,\n",
    "            callbacks = [ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n",
    "                                           verbose=0, mode='auto', min_delta=0.001, \n",
    "                                           cooldown=0, min_lr=0),\n",
    "            EarlyStopping(patience = 5)])\n",
    "        \n",
    "\n",
    "        y_test_status_pred = model.predict(x_test)\n",
    "        pred = np.array(y_test_status_pred[0:time_length])\n",
    "        pred_dead = pred[:, :, 1]\n",
    "\n",
    "        cif1 = pd.DataFrame(pred_dead, np.arange(time_length) + 1)\n",
    "        ev1 = EvalSurv(1-cif1, durations_test//time_interval, events_test == 1, censor_surv='km')\n",
    "        c_index = ev1.concordance_td('antolini')\n",
    "        cindexes.append(c_index)\n",
    "\n",
    "    ssmtlr_cv_results.iloc[index, 4] = np.mean(cindexes)\n",
    "    ssmtlr_cv_results.to_csv('../cv.results.ssmtlr.csv', index = False)\n",
    "    print(ssmtlr_cv_results.iloc[index, ].values)\n",
    "\n",
    "\n",
    "\n",
    "ssmtlr_cv_results = pd.read_csv(\"../cv.results.ssmtlr.csv\")\n",
    "print(ssmtlr_cv_results[\"cindex\"].values.max())\n",
    "ind_best = ssmtlr_cv_results[\"cindex\"].values.argmax()\n",
    "\n",
    "lambda3 = ssmtlr_cv_results.iloc[ind_best, 0]\n",
    "lambda4 = ssmtlr_cv_results.iloc[ind_best, 1]\n",
    "lr = ssmtlr_cv_results.iloc[ind_best, 2]\n",
    "batch_size = ssmtlr_cv_results.iloc[ind_best, 3]\n",
    "\n",
    "\n",
    "input_tensor = Input((x_train.shape[1],))\n",
    "x = input_tensor\n",
    "x = Dense(16, activation = 'sigmoid', kernel_regularizer = L1L2(l1 = 0., l2 = 0.),\n",
    "    kernel_initializer= tf.keras.initializers.VarianceScaling())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(8, activation = 'sigmoid', kernel_regularizer = L1L2(l1 = 0., l2 = 0.),\n",
    "    kernel_initializer= tf.keras.initializers.VarianceScaling())(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prepare_list = {}\n",
    "for i in range(time_length):\n",
    "     prepare_list['x' + str(i)] = Dense(3, activation = 'softmax', kernel_regularizer = L1L2(l1 = 0., l2 = 0.), \n",
    "                                        name = 'month_' + str(i))(x)\n",
    "\n",
    "xx1 = concatenate(list(prepare_list.values()))\n",
    "xx2 = Lambda(lambda x: x[:, 1::3], name = 'ranking_cause')(xx1)\n",
    "xx3 = Lambda(lambda x: x[:, 2::3], name = 'ranking_other')(xx1)\n",
    "\n",
    "losses = {}\n",
    "loss_weights = {}\n",
    "for i in range(time_length):\n",
    "    losses['month_' + str(i)] = logloss(lambda3)\n",
    "    loss_weights['month_' + str(i)] = 1\n",
    "\n",
    "losses['ranking_cause'] = rankingloss_cause\n",
    "losses['ranking_other'] = rankingloss_other\n",
    "loss_weights['ranking_cause'] = lambda4\n",
    "loss_weights['ranking_other'] = lambda4\n",
    "\n",
    "\n",
    "\n",
    "model = Model(input_tensor, list(prepare_list.values()) + [xx2] + [xx3])\n",
    "model.compile(optimizer = Adam(lr), loss = losses, loss_weights = loss_weights)\n",
    "\n",
    "model.fit(x_train, y_train_status_f, epochs = 100, validation_data=(x_test, y_test_status_f), \n",
    "          batch_size = batch_size, shuffle = True, verbose = 1,\n",
    "          callbacks = [\n",
    "          ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=0),\n",
    "          EarlyStopping(patience = 5)])\n",
    "\n",
    "\n",
    "y_test_status_pred = model.predict(x_test)\n",
    "pred = np.array(y_test_status_pred[0:time_length])\n",
    "pred_dead = pred[:, :, 1]\n",
    "\n",
    "\n",
    "\n",
    "cif1 = pd.DataFrame(pred_dead, np.arange(time_length) + 1)\n",
    "ev1 = EvalSurv(1-cif1, durations_test//time_interval, events_test == 1, censor_surv='km')\n",
    "c_index = ev1.concordance_td('antolini')\n",
    "ibs = ev1.integrated_brier_score(np.arange(time_length))\n",
    "\n",
    " \n",
    "\n",
    "print('C-index: {:.4f}'.format(c_index))\n",
    "print('IBS: {:.4f}'.format(ibs))\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------- bootstrap -------------------------------------\n",
    "def bootstrap_replicate_1d(data):\n",
    "    bs_sample = np.random.choice(data,len(data))\n",
    "    return bs_sample\n",
    "\n",
    "\n",
    "bootstrap_R = 100\n",
    "c_indexes = []\n",
    "ibss = []\n",
    "\n",
    "\n",
    "for i in range(bootstrap_R):\n",
    "    print(i)\n",
    "    train_bs_idx = bootstrap_replicate_1d(np.array(range(x_train.shape[0])))\n",
    "    X_tr = x_train[train_bs_idx, ]\n",
    "    Y_tr_0 = y_train[train_bs_idx, ]\n",
    "\n",
    "    Y_tr_1 = []\n",
    "    for i in range(time_length):\n",
    "        Y_tr_1.append(y_train_status[i][train_bs_idx])\n",
    "    \n",
    "    Y_tr = Y_tr_1 + [Y_tr_0] + [Y_tr_0]\n",
    "\n",
    "    input_tensor = Input((X_tr.shape[1],))\n",
    "    x = input_tensor\n",
    "    x = Dense(16, activation = 'sigmoid', kernel_regularizer = L1L2(l1 = 0., l2 = 0.),\n",
    "    kernel_initializer= tf.keras.initializers.VarianceScaling())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(8, activation = 'sigmoid', kernel_regularizer = L1L2(l1 = 0., l2 = 0.),\n",
    "    kernel_initializer= tf.keras.initializers.VarianceScaling())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    prepare_list = {}\n",
    "    for i in range(time_length):\n",
    "         prepare_list['x' + str(i)] = Dense(3, activation = 'softmax', \n",
    "                                            kernel_regularizer = L1L2(l1 = 0., l2 = 0.), \n",
    "                                            name = 'month_' + str(i))(x)\n",
    "\n",
    "    xx1 = concatenate(list(prepare_list.values()))\n",
    "    xx2 = Lambda(lambda x: x[:, 1::3], name = 'ranking_cause')(xx1)\n",
    "    xx3 = Lambda(lambda x: x[:, 2::3], name = 'ranking_other')(xx1)\n",
    "\n",
    "    losses = {}\n",
    "    loss_weights = {}\n",
    "    for i in range(time_length):\n",
    "        losses['month_' + str(i)] = logloss(lambda3)\n",
    "        loss_weights['month_' + str(i)] = 1\n",
    "\n",
    "    losses['ranking_cause'] = rankingloss_cause\n",
    "    losses['ranking_other'] = rankingloss_other\n",
    "    loss_weights['ranking_cause'] = lambda4\n",
    "    loss_weights['ranking_other'] = lambda4\n",
    "\n",
    "    model = Model(input_tensor, list(prepare_list.values()) + [xx2] + [xx3])\n",
    "    model.compile(optimizer = Adam(lr),\n",
    "                  loss = losses,\n",
    "                  loss_weights = loss_weights)\n",
    "    model.fit(X_tr, Y_tr, epochs = 100, validation_data=(x_test, y_test_status_f), \n",
    "              batch_size = batch_size, shuffle = True, verbose = 0,\n",
    "              callbacks = [\n",
    "              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=0, mode='auto', \n",
    "                                min_delta=0.001, cooldown=0, min_lr=0),\n",
    "              EarlyStopping(patience = 5)])\n",
    "\n",
    "    y_test_status_pred = model.predict(x_test)\n",
    "    pred = np.array(y_test_status_pred[0:time_length])\n",
    "    pred_dead = pred[:, :, 1]\n",
    "\n",
    "    cif1 = pd.DataFrame(pred_dead, np.arange(time_length) + 1)\n",
    "    ev1 = EvalSurv(1-cif1, durations_test//time_interval, events_test == 1, censor_surv='km')\n",
    "    c_index = ev1.concordance_td('antolini')\n",
    "    ibs = ev1.integrated_brier_score(np.arange(time_length))\n",
    "\n",
    "   \n",
    "\n",
    "    c_indexes.append(c_index)\n",
    "    ibss.append(ibs)\n",
    "    print('C-index: {:.4f}'.format(c_index))\n",
    "    print('IBS: {:.4f}'.format(ibs))\n",
    "\n",
    "\n",
    "pd.DataFrame(data = {\"cindex\": c_indexes, \"ibs\": ibss}).to_csv(\"../results.ci.ssmtlr.csv\", index=False)\n",
    "\n",
    "# Compute the 95% confidence interval: conf_int\n",
    "mean_cindex = np.mean(c_indexes)\n",
    "mean_ibs = np.mean(ibss)\n",
    "\n",
    "# Print the mean\n",
    "print('mean cindex =', mean_cindex)\n",
    "print('mean ibs =', mean_ibs)\n",
    "\n",
    "\n",
    "ci_cindex = np.percentile(c_indexes, [2.5, 97.5])\n",
    "ci_ibs = np.percentile(ibss, [2.5, 97.5])\n",
    " \n",
    "# Print the confidence interval\n",
    "print('confidence interval =', ci_cindex)\n",
    "print('confidence interval =', ci_ibs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
