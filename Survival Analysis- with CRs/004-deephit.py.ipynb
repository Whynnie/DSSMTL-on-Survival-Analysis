{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "\n",
    "from pycox.preprocessing.label_transforms import LabTransDiscreteTime\n",
    "from pycox.models import DeepHit\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "\n",
    "np.random.seed(1234)\n",
    "_ = torch.manual_seed(1234)\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(\"../Melanoma_train_data_py.csv\")\n",
    "df_test = pd.read_csv(\"../Melanoma_test_data_py.csv\")\n",
    "\n",
    "## Dealing with Missing values\n",
    "\n",
    "#df_train['meal.cal'].fillna(df_train['meal.cal'].mean(), inplace=True)\n",
    "#df_train['wt.loss'].fillna(df_train['wt.loss'].mean(), inplace=True)\n",
    "#df_test['meal.cal'].fillna(df_test['meal.cal'].mean(), inplace=True)\n",
    "#df_test['wt.loss'].fillna(df_test['wt.loss'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "\n",
    "get_x = lambda df: (df.drop(columns=['time', 'status']).values.astype('float32'))\n",
    "x_train = get_x(df_train)\n",
    "x_test = get_x(df_test)\n",
    "\n",
    "\n",
    "class LabTransform(LabTransDiscreteTime):\n",
    "    def transform(self, durations, events):\n",
    "        durations, is_event = super().transform(durations, events > 0)\n",
    "        events[is_event == 0] = 0\n",
    "        return durations, events.astype('int64')\n",
    "\n",
    "\n",
    "num_durations = 108\n",
    "labtrans = LabTransform(num_durations)\n",
    "get_target = lambda df: (df['time'].values, df['status'].values)\n",
    "\n",
    "\n",
    "y_train = labtrans.fit_transform(*get_target(df_train))\n",
    "y_test = labtrans.fit_transform(*get_target(df_test))\n",
    "durations_test, events_test = get_target(df_test)\n",
    "\n",
    "\n",
    "class CauseSpecificNet(torch.nn.Module):\n",
    "    \"\"\"Network structure similar to the DeepHit paper, but without the residual\n",
    "    connections (for simplicity).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, num_nodes_shared, num_nodes_indiv, num_risks,\n",
    "                 out_features, batch_norm=True, dropout=None):\n",
    "        super().__init__()\n",
    "        self.shared_net = tt.practical.MLPVanilla(\n",
    "            in_features, num_nodes_shared[:-1], num_nodes_shared[-1],\n",
    "            batch_norm, dropout,\n",
    "        )\n",
    "        self.risk_nets = torch.nn.ModuleList()\n",
    "        for _ in range(num_risks):\n",
    "            net = tt.practical.MLPVanilla(\n",
    "                num_nodes_shared[-1], num_nodes_indiv, out_features,\n",
    "                batch_norm, dropout,\n",
    "            )\n",
    "            self.risk_nets.append(net)\n",
    "    def forward(self, input):\n",
    "        out = self.shared_net(input)\n",
    "        out = [net(out) for net in self.risk_nets]\n",
    "        out = torch.stack(out, dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "in_features = x_train.shape[1]\n",
    "num_risks = y_train[1].max()\n",
    "out_features = len(labtrans.cuts)\n",
    "dropout = [0.0]\n",
    "\n",
    "\n",
    "\n",
    "list_num_nodes_shared = [[32, 8], [16, 8], [16, 4], [8, 4]]\n",
    "list_num_nodes_indiv = [[32], [16], [8]]\n",
    "list_batch_norm = [False, True]\n",
    "list_lr = [0.1, 0.01, 0.001, 0.0001]\n",
    "list_alpha = [0.1, 0.2, 0.3, 0.4]\n",
    "list_sigma = [0.1, 0.2, 0.3, 0.4]\n",
    "list_batch_size = [128, 256]\n",
    "\n",
    "\n",
    "parameters = []\n",
    "for num_nodes_shared in list_num_nodes_shared:\n",
    "    for num_nodes_indiv in list_num_nodes_indiv:\n",
    "        for batch_norm in list_batch_norm:\n",
    "            for lr in list_lr:\n",
    "                for alpha in list_alpha:\n",
    "                    for sigma in list_sigma:\n",
    "                        for batch_size in list_batch_size:\n",
    "                            parameters.append([num_nodes_shared, num_nodes_indiv, batch_norm, lr, alpha, sigma, batch_size])\n",
    "\n",
    "\n",
    "\n",
    "deephit_cv_results = pd.DataFrame(parameters)\n",
    "deephit_cv_results[\"cindex\"] = 0\n",
    "\n",
    "kf = KFold(n_splits = 5)\n",
    "\n",
    "\n",
    "for index in range(deephit_cv_results.shape[0]):\n",
    "    num_nodes_shared = deephit_cv_results.iloc[index, 0]\n",
    "    num_nodes_indiv = deephit_cv_results.iloc[index, 1]\n",
    "    batch_norm = deephit_cv_results.iloc[index, 2]\n",
    "    lr = deephit_cv_results.iloc[index, 3]\n",
    "    alpha = deephit_cv_results.iloc[index, 4]\n",
    "    sigma = deephit_cv_results.iloc[index, 5]\n",
    "    batch_size = deephit_cv_results.iloc[index, 6]\n",
    "    \n",
    "    cindexes = []\n",
    "    for train_index, test_index in kf.split(df_train):\n",
    "        X_tr = x_train[train_index, ]\n",
    "        X_val = x_train[test_index, ]\n",
    "        Y_tr_0 = y_train[0][train_index, ]\n",
    "        Y_tr_1 = y_train[1][train_index, ]\n",
    "        Y_val_0 = y_train[0][test_index, ]\n",
    "        Y_val_1 = y_train[1][test_index, ]\n",
    "        Y_tr = (Y_tr_0, Y_tr_1)\n",
    "        Y_val = (Y_val_0, Y_val_1)\n",
    "        \n",
    "        # net = SimpleMLP(in_features, num_nodes_shared, num_risks, out_features)\n",
    "        net = CauseSpecificNet(in_features, num_nodes_shared, num_nodes_indiv, \n",
    "                               num_risks, out_features, batch_norm, dropout)\n",
    "        optimizer = tt.optim.AdamWR(lr = lr, decoupled_weight_decay = 0.01,\n",
    "                            cycle_eta_multiplier=0.8)\n",
    "        model = DeepHit(net, optimizer, alpha = alpha, sigma = sigma,\n",
    "                        duration_index = labtrans.cuts)\n",
    "\n",
    "        epochs = 100\n",
    "        callbacks = [tt.callbacks.EarlyStoppingCycle()]\n",
    "        verbose = False # set to True if you want printout\n",
    "\n",
    "        log = model.fit(X_tr, Y_tr, int(batch_size), epochs,\n",
    "                callbacks, verbose, val_data = (X_val, Y_val))\n",
    "        \n",
    "        cif = model.predict_cif(x_test)\n",
    "        cif1 = pd.DataFrame(cif[0], model.duration_index)\n",
    "        ev1 = EvalSurv(1-cif1, durations_test, events_test == 1, censor_surv='km')\n",
    "        c_index = ev1.concordance_td()\n",
    "\n",
    "        cindexes.append(c_index)\n",
    "\n",
    "    deephit_cv_results.iloc[index, 7] = np.mean(cindexes)\n",
    "    deephit_cv_results.to_csv('../cv.results.deephit.csv', index = False)\n",
    "    print(deephit_cv_results.iloc[index, ].values)\n",
    "\n",
    "deephit_cv_results = pd.read_csv(\"../cv.results.deephit.csv\")\n",
    "print(deephit_cv_results[\"cindex\"].values.max())\n",
    "ind_best = deephit_cv_results[\"cindex\"].values.argmax()\n",
    "\n",
    "num_nodes_shared = eval(deephit_cv_results.iloc[ind_best, 0])\n",
    "num_nodes_indiv = eval(deephit_cv_results.iloc[ind_best, 1])\n",
    "batch_norm = deephit_cv_results.iloc[ind_best, 2]\n",
    "lr = deephit_cv_results.iloc[ind_best, 3]\n",
    "\n",
    "alpha = deephit_cv_results.iloc[ind_best, 4]\n",
    "sigma = deephit_cv_results.iloc[ind_best, 5]\n",
    "batch_size = deephit_cv_results.iloc[ind_best, 6]\n",
    "\n",
    "\n",
    "\n",
    "net = CauseSpecificNet(in_features, num_nodes_shared, num_nodes_indiv, num_risks, out_features, batch_norm, dropout)\n",
    "optimizer = tt.optim.AdamWR(lr = lr, decoupled_weight_decay = 0.01, cycle_eta_multiplier=0.8)\n",
    "model = DeepHit(net, optimizer, alpha = alpha, sigma = sigma, duration_index = labtrans.cuts)\n",
    "        \n",
    "\n",
    "epochs = 100\n",
    "callbacks = [tt.callbacks.EarlyStoppingCycle()]\n",
    "log = model.fit(x_train, y_train, int(batch_size), epochs, callbacks, val_data = (x_test, y_test))\n",
    "\n",
    "cif = model.predict_cif(x_test)\n",
    "cif1 = pd.DataFrame(cif[0], model.duration_index)\n",
    "ev = EvalSurv(1-cif1, durations_test, events_test == 1, censor_surv='km')\n",
    "\n",
    "c_index = ev.concordance_td('antolini')\n",
    "print('C-index: {:.4f}'.format(c_index))\n",
    "\n",
    "\n",
    "time_grid = np.linspace(df_test[\"time\"].values.min(), df_test[\"time\"].values.max(), 100)\n",
    "ibs = ev.integrated_brier_score(time_grid) \n",
    "print('IBS: {:.4f}'.format(ibs))\n",
    "\n",
    "\n",
    "def bootstrap_replicate_1d(data):\n",
    "    bs_sample = np.random.choice(data,len(data))\n",
    "    return bs_sample\n",
    "\n",
    "\n",
    "bootstrap_R = 100\n",
    "c_indexes = []\n",
    "ibss = []\n",
    "\n",
    "\n",
    "for i in range(bootstrap_R):\n",
    "    print(i)\n",
    "    train_bs_idx = bootstrap_replicate_1d(np.array(range(df_train.shape[0])))\n",
    "    # Creating the X, T and E input\n",
    "    X_train = x_train[train_bs_idx, ]\n",
    "    T_train = y_train[0][train_bs_idx]\n",
    "    E_train = y_train[1][train_bs_idx]\n",
    "    Y_train = (T_train, E_train)\n",
    "    net = CauseSpecificNet(in_features, num_nodes_shared, num_nodes_indiv, num_risks, out_features, batch_norm, dropout)\n",
    "    optimizer = tt.optim.AdamWR(lr = lr, decoupled_weight_decay = 0.01, cycle_eta_multiplier=0.8)\n",
    "    model = DeepHit(net, optimizer, alpha = alpha, sigma = sigma, duration_index = labtrans.cuts)\n",
    "    epochs = 100\n",
    "    callbacks = [tt.callbacks.EarlyStoppingCycle()]\n",
    "    log = model.fit(x_train, y_train, int(batch_size), epochs, callbacks, val_data = (x_test, y_test))\n",
    "    cif = model.predict_cif(x_test)\n",
    "    cif1 = pd.DataFrame(cif[0], model.duration_index)\n",
    "    ev = EvalSurv(1-cif1, durations_test, events_test == 1, censor_surv='km')\n",
    "    # ev = EvalSurv(surv, Y_val_0, Y_val_1, censor_surv='km')\n",
    "    c_index = ev.concordance_td('antolini')\n",
    "    time_grid = np.linspace(df_test[\"time\"].values.min(), df_test[\"time\"].values.max(), 108)\n",
    "    ibs = ev.integrated_brier_score(time_grid) \n",
    "    c_indexes.append(np.round(c_index, 4))\n",
    "    ibss.append(np.round(ibs, 4))\n",
    "\n",
    "\n",
    "pd.DataFrame(data = {\"cindex\": c_indexes, \"ibs\": ibss}).to_csv(\"../results.ci.deephit.csv\", index=False)\n",
    "\n",
    "# Compute the 95% confidence interval: conf_int\n",
    "mean_cindex = np.mean(c_indexes)\n",
    "mean_ibs = np.mean(ibss)\n",
    "\n",
    "\n",
    "# Print the mean\n",
    "print('mean cindex =', mean_cindex)\n",
    "print('mean ibs =', mean_ibs)\n",
    "\n",
    "\n",
    "ci_cindex = np.percentile(c_indexes, [2.5, 97.5])\n",
    "ci_ibs = np.percentile(ibss, [2.5, 97.5])\n",
    "\n",
    "# Print the confidence interval\n",
    "print('confidence interval =', ci_cindex)\n",
    "print('confidence interval =', ci_ibs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
