{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c316b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import ndimage\n",
    "from sklearn import metrics\n",
    "\n",
    "from lifelines.utils import concordance_index\n",
    "from pycox.evaluation import EvalSurv\n",
    "from pycox.preprocessing.label_transforms import LabTransDiscreteTime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras import backend as k\n",
    "from sklearn.preprocessing import normalize\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(\"birth control_train_data_py.csv\", index_col= 0)\n",
    "df_test = pd.read_csv(\"birth control_test_data_py.csv\",index_col= 0)\n",
    "\n",
    "# The Correlation of the features\n",
    "\n",
    "sns.heatmap(df_train.corr(), annot = True, cmap = \"YlGnBu\")\n",
    "tf.random.set_seed(2020)\n",
    "\n",
    "\n",
    "## Performing OneHotEncoding\n",
    "df_train[\"homeStyle\"] = df_train[\"homeStyle\"].apply(lambda x:1 if x==\"urban\" else 0)\n",
    "df_test[\"status\"] = df_test[\"status\"].apply(lambda x:1 if x==2 else 0)\n",
    "\n",
    "\n",
    "## Dealing with missing values\n",
    "df_train[\"meal.cal\"].fillna(df_train[\"meal.cal\"].mean(), inplace=True)\n",
    "df_train[\"wt.loss\"].fillna(df_train[\"wt.loss\"].mean(), inplace=True)\n",
    "df_test[\"pat.karno\"].fillna(df_train[\"pat.karno\"].mean(), inplace=True)\n",
    "df_test[\"meal.cal\"].fillna(df_train[\"meal.cal\"].mean(), inplace=True)\n",
    "df_test[\"wt.loss\"].fillna(df_train[\"wt.loss\"].mean(), inplace=True)\n",
    "df_train.dropna(inplace = True)\n",
    "\n",
    "\n",
    "get_x = lambda df: (df.drop(columns=['time', 'status']).values.astype('float32'))\n",
    "x_train = get_x(df_train)\n",
    "x_test = get_x(df_test)\n",
    "\n",
    "\n",
    "## Mult task transformation:\n",
    "def get_y_labels(status, time):\n",
    "    ret = np.ones((status.shape[0], np.max(time) + 1)) \n",
    "    for i in range(status.shape[0]):\n",
    "        if status[i] == 1:\n",
    "            ret[i, 0:time[i] - 1 + 1] = 0\n",
    "        elif status[i] == 0:\n",
    "            ret[i, 0:time[i] + 1] = 0\n",
    "            ret[i, time[i] + 1:] = 2        \n",
    "    return ret\n",
    "\n",
    "y_train = get_y_labels(df_train['status'], df_train['time'])\n",
    "y_test = get_y_labels(df_test['status'], df_test['time'])\n",
    "\n",
    "time_interval = 6\n",
    "time_max = np.max(df_train['time'])\n",
    "time_length = time_max//time_interval\n",
    "time_max_test = np.max(df_test['time'])\n",
    "time_length_test = time_max_test//time_interval\n",
    "\n",
    "\n",
    "y_train = y_train[:, np.arange(time_interval, time_max, time_interval)]\n",
    "y_test = y_test[:, np.arange(time_interval, time_max_test, time_interval)]\n",
    "\n",
    "print (\"train_set_x shape: \" + str(x_train.shape))\n",
    "print (\"train_set_y shape: \" + str(y_train.shape))\n",
    "print (\"test_set_x shape: \" + str(x_test.shape))\n",
    "print (\"test_set_y shape: \" + str(y_test.shape))\n",
    "\n",
    "y_train_status = to_categorical(y_train)\n",
    "y_test_status = to_categorical(y_test)\n",
    "\n",
    "\n",
    "def reshape_y(y):\n",
    "    dim = y.shape[1]\n",
    "    ret = []\n",
    "    for i in range(dim):\n",
    "        ret.append(y[:, i, 0:2])\n",
    "    return ret        \n",
    "\n",
    "y_train_status = reshape_y(y_train_status)\n",
    "y_test_status = reshape_y(y_test_status)\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "y_train_status_f = y_train_status + [y_train]\n",
    "y_test_status_f = y_test_status + [y_test]\n",
    "\n",
    "\n",
    "get_target = lambda df: (df['time'].values, df['status'].values)\n",
    "durations_train, events_train = get_target(df_train)\n",
    "durations_test, events_test = get_target(df_test)\n",
    "\n",
    "pd.DataFrame(y_test)\n",
    "\n",
    "def logloss(lambda3):\n",
    "    def loss(y_true, y_pred):\n",
    "        mask_dead = y_true[:, 1]\n",
    "        mask_alive = y_true[:, 0]\n",
    "        mask_censored = 1 - (mask_alive + mask_dead)\n",
    "        logloss = -1 * k.mean(mask_dead * k.log(y_pred[:, 1]) + mask_alive * k.log(y_pred[:, 0]))\n",
    "        - lambda3 * k.mean(y_pred[:, 1] * mask_censored * k.log(y_pred[:, 1])) # / 39899\n",
    "        return logloss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def rankingloss(y_true, y_pred, name = None):\n",
    "    ranking_loss = 0\n",
    "    for i in range(time_length):\n",
    "        for j in range(i + 1, time_length, 1):\n",
    "            tmp = y_pred[:, i] - y_pred[:, j]\n",
    "            tmp1 = tmp > 0\n",
    "            tmp1 = tf.cast(tmp1, tf.float32)\n",
    "            ranking_loss = ranking_loss + k.mean((tmp1 * tmp * (j - i)))\n",
    "    return ranking_loss\n",
    "\n",
    "\n",
    "\n",
    "list_lambda3 = [0.1,  0.4, 0.7, 0.9]\n",
    "list_lambda4 = [0.01, 0.1, 1]\n",
    "list_lr = [0.1, 0.01, 0.001]\n",
    "list_batch_size = [64]\n",
    "\n",
    "parameters = []\n",
    "for lambda3 in list_lambda3:\n",
    "    for lambda4 in list_lambda4:\n",
    "        for lr in list_lr:\n",
    "            for batch_size in list_batch_size:\n",
    "                parameters.append([lambda3, lambda4, lr, batch_size])\n",
    "\n",
    "ssmtlr_cv_results = pd.DataFrame(parameters)\n",
    "ssmtlr_cv_results[\"cindex\"] = 0\n",
    "\n",
    "kf = KFold(n_splits = 3)\n",
    "\n",
    "\n",
    "for index in range(ssmtlr_cv_results.shape[0]):\n",
    "    lambda3 = ssmtlr_cv_results.iloc[index, 0]\n",
    "    lambda4 = ssmtlr_cv_results.iloc[index, 1]\n",
    "    lr = ssmtlr_cv_results.iloc[index, 2]\n",
    "    batch_size = ssmtlr_cv_results.iloc[index, 3]\n",
    "\n",
    "    cindexes = []\n",
    "    for train_index, test_index in kf.split(df_train):\n",
    "        X_tr = x_train[train_index, ]\n",
    "        X_val = x_train[test_index, ]\n",
    "\n",
    "        Y_tr_0 = y_train[train_index, ]\n",
    "        Y_val_0 = y_train[test_index, ]\n",
    "\n",
    "        Y_tr_1 = []\n",
    "        Y_val_1 = []\n",
    "        for i in range(time_length):\n",
    "            Y_tr_1.append(y_train_status[i][train_index])\n",
    "            Y_val_1.append(y_train_status[i][test_index])\n",
    "        \n",
    "        Y_tr = Y_tr_1 + [Y_tr_0]\n",
    "        Y_val = Y_val_1 + [Y_val_0]\n",
    "\n",
    "        input_tensor = Input((X_tr.shape[1],))\n",
    "        x = input_tensor\n",
    "        x = Dense(24, activation = 'sigmoid', kernel_regularizer = L1L2(l1 = 0., l2 = 0.))(x)\n",
    "        x = Dense(16, activation = 'sigmoid', kernel_regularizer = L1L2(l1 = 0., l2 = 0.))(x)\n",
    "        x = Dense(6, activation = 'sigmoid', kernel_regularizer = L1L2(l1 = 0., l2 = 0.))(x)\n",
    "\n",
    "        prepare_list = {}\n",
    "        for i in range(time_length):\n",
    "             prepare_list['x' + str(i)] = Dense(2, activation = 'softmax', \n",
    "                                                kernel_regularizer = L1L2(l1 = 0., l2 = 0.), \n",
    "                                                name = 'month_' + str(i))(x)\n",
    "\n",
    "        xx1 = concatenate(list(prepare_list.values()))\n",
    "        xx2 = Lambda(lambda x: x[:, 1::2], name = 'ranking')(xx1)\n",
    "\n",
    "        losses = {}\n",
    "        loss_weights = {}\n",
    "        for i in range(time_length):\n",
    "            losses['month_' + str(i)] = logloss(lambda3)\n",
    "            loss_weights['month_' + str(i)] = 1\n",
    "        losses['ranking'] = rankingloss\n",
    "        loss_weights['ranking'] = lambda4\n",
    "\n",
    "        model = Model(input_tensor, list(prepare_list.values()) + [xx2])\n",
    "        model.compile(optimizer = Adam(lr),\n",
    "                      loss = losses, # 'categorical_crossentropy',\n",
    "                      loss_weights = loss_weights)\n",
    "        model.fit(X_tr, Y_tr, epochs = 100, validation_data=(X_val, Y_val), \n",
    "                  batch_size = batch_size, shuffle = True, # verbose = 0,\n",
    "                  callbacks = [\n",
    "                  ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=0, mode='auto', \n",
    "                                    min_delta=0.001, cooldown=0, min_lr=0),\n",
    "                  # roc_callback(training_data=(X_train, Y_train_status),validation_data=(X_test, Y_test_status)),\n",
    "                     # LearningRateScheduler(lr_schedule),\n",
    "                      EarlyStopping(patience = 3)])\n",
    "\n",
    "        y_test_status_pred = model.predict(x_test)\n",
    "        pred = np.array(y_test_status_pred[0:time_length])\n",
    "        pred_dead = pred[:, :, 1]\n",
    "\n",
    "        cif1 = pd.DataFrame(pred_dead, np.arange(time_length) + 1)\n",
    "        ev1 = EvalSurv(1-cif1, durations_test//time_interval, events_test == 1, censor_surv='km')\n",
    "        c_index = ev1.concordance_td('antolini')\n",
    "        cindexes.append(c_index)\n",
    "\n",
    "    ssmtlr_cv_results.iloc[index, 4] = np.mean(cindexes)\n",
    "    ssmtlr_cv_results.to_csv('cv.ssmtl_without_CR.results.ssmtlr.csv', index = False)\n",
    "    print(ssmtlr_cv_results.iloc[index, ].values)\n",
    "\n",
    "ssmtlr_cv_results = pd.read_csv(\"cv.ssmtl_without_CR.results.ssmtlr.csv\")\n",
    "print(ssmtlr_cv_results[\"cindex\"].values.max())\n",
    "ind_best = ssmtlr_cv_results[\"cindex\"].values.argmax()\n",
    "\n",
    "lambda3 = ssmtlr_cv_results.iloc[ind_best, 0]\n",
    "lambda4 = ssmtlr_cv_results.iloc[ind_best, 1]\n",
    "lr = ssmtlr_cv_results.iloc[ind_best, 2]\n",
    "batch_size = ssmtlr_cv_results.iloc[ind_best, 3]\n",
    "\n",
    "\n",
    "input_tensor = Input((x_train.shape[1],))\n",
    "x = input_tensor\n",
    "x = Dense(24, activation = 'sigmoid', kernel_regularizer = L1L2(l1 = 0., l2 = 0.))(x)\n",
    "x = Dense(16, activation = 'sigmoid', kernel_regularizer = L1L2(l1 = 0., l2 = 0.))(x)\n",
    "x = Dense(6, activation = 'sigmoid', kernel_regularizer = L1L2(l1 = 0., l2 = 0.))(x)\n",
    "\n",
    "prepare_list = {}\n",
    "for i in range(time_length):\n",
    "     prepare_list['x' + str(i)] = Dense(2, activation = 'softmax', \n",
    "                                        kernel_regularizer = L1L2(l1 = 0., l2 = 0.), \n",
    "                                        name = 'month_' + str(i))(x)\n",
    "\n",
    "xx1 = concatenate(list(prepare_list.values()))\n",
    "xx2 = Lambda(lambda x: x[:, 1::2], name = 'ranking')(xx1)\n",
    "\n",
    "losses = {}\n",
    "loss_weights = {}\n",
    "for i in range(time_length):\n",
    "    losses['month_' + str(i)] = logloss(lambda3)\n",
    "    loss_weights['month_' + str(i)] = 1\n",
    "losses['ranking'] = rankingloss\n",
    "loss_weights['ranking'] = lambda4\n",
    "\n",
    "\n",
    "model = Model(input_tensor, list(prepare_list.values()) + [xx2])\n",
    "model.compile(optimizer = Adam(lr),\n",
    "              loss = losses,\n",
    "              loss_weights = loss_weights)\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train_status_f, epochs = 100, validation_data=(x_test, y_test_status_f), \n",
    "          batch_size = batch_size, shuffle = True, verbose = 0,\n",
    "          callbacks = [\n",
    "          ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=0, mode='auto', \n",
    "                            min_delta=0.001, cooldown=0, min_lr=0),\n",
    "          EarlyStopping(patience = 3)])\n",
    "\n",
    "y_test_status_pred = model.predict(x_test)\n",
    "pred = np.array(y_test_status_pred[0:time_length])\n",
    "pred_dead = pred[:, :, 1]\n",
    "\n",
    "\n",
    "cif1 = pd.DataFrame(pred_dead, np.arange(time_length) + 1)\n",
    "ev1 = EvalSurv(1-cif1, durations_test//time_interval, events_test == 1, censor_surv='km')\n",
    "c_index = ev1.concordance_td('antolini')\n",
    "ibs = ev1.integrated_brier_score(np.arange(time_length))\n",
    "print('C-index: {:.4f}'.format(c_index))\n",
    "print('IBS: {:.4f}'.format(ibs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
